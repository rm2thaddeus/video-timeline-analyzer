/*
üìå Purpose ‚Äì Defines the phased, documentation-first roadmap for the de-novo-windows branch of the Video Timeline Analyzer.
üîÑ Latest Changes ‚Äì Clarified Windows-native focus; removed WSL2/Linux as default; emphasized modular, pipeline-oriented backend and PyTorch-only scene detection.
üîÑ 2024-06: Windows development is now the mainline; Hugging Face weights integration and reproducible download script for TransNetV2 PyTorch weights. Documented hardware-agnostic model loading for Windows.
‚öôÔ∏è Key Logic ‚Äì Backend development is prioritized, with modular scene detection (TransNet V2, PyTorch-only, hardware-agnostic).
üìÇ Expected File Path ‚Äì docs/ROADMAP.md
üß† Reasoning ‚Äì Ensures a rigorous, reproducible, and maintainable backend foundation for Windows-native development before any UI work.
*/

# Project Roadmap (De-Novo-Windows Branch)

## Overview

This roadmap defines the phases and milestones for the de-novo-windows branch, focusing on backend development for **Windows-native** environments (no WSL2 or Linux required). The goal is a modular, pipeline-oriented backend for scientific video analysis, with PyTorch-only scene detection (TransNet V2).

**Note:** This project is being developed by a single scientist-developer, with a focus on learning, fun, and leveraging AI/code assistants to minimize manual coding. The process is designed to be enjoyable and educational, not just productive.

## Scene Detection: PyTorch (TransNetV2) Windows-Native Status

**Current State:**
- Scene detection now uses the PyTorch implementation of TransNetV2, with official weights published on Hugging Face ([link](https://huggingface.co/ByteDance/shot2story/blob/ff853c571fd92eb4e0c5713e27f2a323ac903f67/transnetv2-pytorch-weights.pth)).
- The pipeline is fully Windows-native: works on Windows (CPU or GPU), with automatic hardware detection.
- No TensorFlow, CUDA setup, or WSL2 is required for scene detection. The only requirement is PyTorch (CPU or GPU) and the weights file.
- The weights are not committed to the repository. Instead, use the reproducible script at `src/scene_detection/download_transnetv2_weights.py` to download them automatically.
- The codebase uses hardware-agnostic loading:
  ```python
  import torch
  from transnetv2_pytorch import TransNetV2
  model = TransNetV2()
  state_dict = torch.load("transnetv2-pytorch-weights.pth", map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))
  model.load_state_dict(state_dict)
  model.eval()
  if torch.cuda.is_available():
      model.cuda()
  ```

**Why:**
- This approach removes the strict dependency on TensorFlow/CUDA versions and enables native Windows development, as well as easier onboarding for all users.
- PyTorch is more flexible and widely supported across platforms.

**Recommended Workflow:**
1. Use the `de-novo-windows` branch for Windows-native development.
2. Run the download script to obtain the weights:
   ```bash
   python src/scene_detection/download_transnetv2_weights.py
   ```
3. The pipeline will use GPU if available, else CPU, with no manual configuration needed.

**Pipeline Compatibility Note:**
- PyTorch is also used for Whisper and embedding models, ensuring a unified, flexible backend.
- Native Windows is now the recommended and supported environment for all backend tasks.

## Audio Analysis: Whisper (Modular, Dual Output)

**Current State:**
- Audio analysis now uses the OpenAI Whisper model (default: `small`) for transcription.
- The pipeline outputs:
    - **JSON**: Segment-level, DataFrame-ready output (default, always saved).
    - **SRT**: Optional, for human alignment/review (enabled by argument).
    - **WAV**: Optional, for full audio extraction (enabled by argument).
- The code is modular, hardware-agnostic (CUDA/CPU), and can be extended for additional audio features or models.
- The number of parallel workers is auto-estimated for your GPU and model size (using `estimate_max_workers`), maximizing speed without risking OOM errors.
- All outputs are saved in the `audio_analysis` output directory, with clear naming for each video.

**Why:**
- JSON provides all metadata needed for variable-granularity DataFrame construction, as described in the architecture.
- SRT and WAV are available for review or downstream use, but are not generated by default to save resources.
- The pipeline is optimized for both speed and scientific reproducibility, with all parameters logged and version-controlled.

**Recommended Workflow:**
1. Use the transcription module in `src/audio_analysis/whisper_transcribe.py`.
2. Run on any video in the `data` directory; outputs will be saved as both `.srt` and `.json`.
3. The JSON output is directly ingestible for DataFrame construction and Qdrant ingestion.

**Pipeline Compatibility Note:**
- The modular approach ensures compatibility with the rest of the backend pipeline and future UI work.

## Visual Analysis: Embedding Extraction

**Current State:**
- Visual embedding extraction now uses a robust, manually preprocessed Hugging Face TimeSformer module (see DEVELOPMENT_SETUP.md). This avoids dependency on processor classes and ensures future compatibility.

**Why:**
- This approach provides a reliable and consistent method for embedding extraction, which is crucial for various downstream tasks.

**Recommended Workflow:**
1. Use the embedding extraction module in `src/visual_analysis/embedding_extraction.py`.
2. Run on any video in the `data` directory; outputs will be saved in the `embedding_extraction` output directory.

**Pipeline Compatibility Note:**
- The modular approach ensures compatibility with the rest of the backend pipeline and future UI work.

---

## Windows-Native Development: Best Practices & Workflow

To ensure full GPU acceleration, reproducibility, and access to the latest scientific Python tools, all backend development should be performed natively in Windows. Follow these guidelines for a seamless workflow:

### 1. Launching Your Windows Development Environment
- Open your project folder in VSCode or Cursor on Windows.
- Ensure your Python environment is set up using a virtual environment (venv) in Windows.

### 2. Project Location & File Access
- Clone your repository inside your Windows user directory (e.g., `C:\Users\<your-username>\Video_Timeline`).
- All file access and development should be performed natively in Windows for best compatibility.

### 3. Python Environment Management
- Always use a Python virtual environment (`venv`) for all development:
  ```powershell
  python -m venv venv
  .\venv\Scripts\activate
  pip install --upgrade pip
  pip install -r requirements.txt
  ```
- Install all dependencies inside the venv. For GPU support, install the latest PyTorch (Windows-compatible).

### 4. GPU & CUDA Verification
- Confirm GPU access with:
  ```powershell
  python -c "import torch; print('Torch CUDA available:', torch.cuda.is_available())"
  ```
- If you see `True`, you are ready for accelerated development.

### 5. Using AI/Code Assistants
- AI/code assistants (Cursor, Copilot, ChatGPT, etc.) work seamlessly in VSCode/Cursor on Windows.
- All code generation, refactoring, and automation should be performed in the Windows environment for consistency.

### 6. Git & Version Control
- Use `git` in Windows for all commits, branching, and pushes.
- Ensure all changes are staged and committed from within Windows to avoid line ending or permission issues.

### 7. Troubleshooting & Documentation
- Record your environment details (PyTorch, CUDA versions) in `docs/ENVIRONMENT.md` for reproducibility.

### 8. Summary Table
| Task Type                | Where to Run?         | Notes                         |
|--------------------------|-----------------------|-------------------------------|
| Backend Python code      | Windows               | Full GPU, best practice       |
| PyTorch                  | Windows               | Required for GPU support      |
| File access (project)    | Windows               | Use Windows FS for best results |
| Git/version control      | Windows               | Avoids Windows/Unix issues    |

> **All backend, scientific, and GPU-accelerated work should be done natively in Windows for maximum compatibility, performance, and reproducibility.**

---

## Phase 1: Documentation & Design

- [x] Review and refine system architecture ([ARCHITECTURE.md](ARCHITECTURE.md)), emphasizing the DataFrame-centric, variable-granularity approach.
- [x] Complete technical specifications ([SPECIFICATIONS.md](SPECIFICATIONS.md)), including data models and alignment logic.
- [x] Finalize development environment and setup documentation ([DEVELOPMENT_SETUP.md](DEVELOPMENT_SETUP.md)).
- [x] Review and update all supporting documentation (e.g., [POWERSHELL_COMMANDS.md](POWERSHELL_COMMANDS.md), [DEVELOPMENT.md](DEVELOPMENT.md)).
- [x] Document all scientific and reproducibility requirements.

**Milestone:** All documentation reviewed and approved by project stakeholders.

---

## Phase 2: Project Rule Creation

- [x] Draft comprehensive Cursor project rules (see `.cursor/rules/cursorrules.mdc`).
- [x] Review and refine rules for:
    - Code structure and modularity
    - Documentation and commenting standards
    - Testing and validation requirements
    - Environment and dependency management
    - Git/GitHub workflow and commit policies
    - Scientific rigor and reproducibility
    - Data management and privacy
- [x] Approve and publish rules in the repository.

**Milestone:** Project rules finalized and published.

---

## Phase 3: Review & Planning for Implementation

- [x] Hold review meeting(s) to ensure all documentation and rules are clear, complete, and actionable.
- [x] Identify any gaps or ambiguities in the design or rules.
- [x] Plan the transition to implementation (future phases to be defined after review).

**Milestone:** Project is ready for codebase bootstrapping, with a clear, documented, and rule-driven foundation.

---

## Phase 4: Backend Bootstrapping & Initial Development

- [x] Set up the modular folder structure (see `src/` and subfolders for each pipeline stage)
- [x] Begin implementation of core backend modules:
    - [x] Ingestion (video/audio loading) ‚Äì started and tested
    - [x] Scene detection (PyTorch-only, TransNet V2)
    - [x] Audio analysis (modular, dual output: SRT + JSON, DataFrame-ready)
    - [x] Visual embedding extraction now uses a robust, manually preprocessed Hugging Face TimeSformer module (see DEVELOPMENT_SETUP.md)
    - [ ] Metadata/DataFrame construction
    - [ ] Database (Qdrant) integration
- [ ] Write initial unit tests in `tests/`
- [ ] Ensure all code follows project rules for modularity, documentation, and reproducibility
- [ ] Commit and push all changes with clear, descriptive messages

**Milestone:** Core backend modules bootstrapped and under version control, ready for iterative development.

---

## Review & Feedback

- Regular review meetings at the end of each phase (self-review or with AI assistant).
- Issues and suggestions tracked via GitHub.
- All major decisions documented for reproducibility.

---

## Tips & References for Solo, AI-Assisted Scientific Development

- **Leverage AI/code assistants** (like Cursor, Copilot, or ChatGPT) for code generation, refactoring, and documentation.
- **Keep learning fun:** Try new tools, experiment, and don't be afraid to iterate or refactor.
- **Document your process:** Use README files, comments, and commit messages to track your learning and decisions.
- **Useful resources:**
    - [The Turing Way: Guide to Reproducible Research](https://the-turing-way.netlify.app/)
    - [Software Carpentry: Scientific Programming Best Practices](https://software-carpentry.org/lessons/)
    - [GitHub Guides: Mastering Markdown](https://guides.github.com/features/mastering-markdown/)
    - [Qdrant Documentation](https://qdrant.tech/documentation/)
    - [TransNet V2 (Shot Boundary Detection)](https://github.com/soCzech/TransNetV2)
    - [OpenAI Whisper](https://github.com/openai/whisper)
    - [CLIP and BLIP-2 Models](https://github.com/openai/CLIP), (https://github.com/salesforce/BLIP)

---

## Next Logical Step

**Ensure Cursor and Code Assistant Plugins Operate in Windows**
- To guarantee all code generation, automation, and AI-assisted development occurs in the correct (GPU-accelerated, Windows-native) environment, make sure Cursor and any code assistant plugins are running in your Windows shell.
- **How to do this:**
  - Open your project folder in VSCode or Cursor on Windows.
  - Confirm that the integrated terminal and all code execution are happening in the Windows environment (you should see your Windows username and path in the prompt, e.g., `C:\Users\aitor>`).
  - If using plugins, ensure they are enabled and configured for the Windows context.
- This step is essential for reproducibility, full GPU acceleration, and seamless scientific development.

*This step ensures that all future development, automation, and troubleshooting are performed in the intended environment, maximizing reliability and scientific rigor.*

*This roadmap is a living document and will be updated as the project evolves. The backend-first, documentation- and rule-driven approach is central to all phases and milestones. Enjoy the journey!*

model = TransNetV2()
state_dict = torch.load("transnetv2-pytorch-weights.pth")
model.load_state_dict(state_dict)
model.eval().cuda()

## Windows Development Branch: de-novo-windows

This branch enables native Windows development by leveraging the PyTorch weights for TransNetV2 published on Hugging Face. The pipeline can now run on Windows (CPU or GPU) without WSL2, provided the weights are downloaded using the provided script.

**Key steps:**
- Use the `de-novo-windows` branch for all Windows-native work.
- Download the TransNetV2 weights with the script at `src/scene_detection/download_transnetv2_weights.py`.
- The pipeline will automatically use GPU if available, else CPU.

**Reproducibility:**
- The weights are not committed to the repository. Instead, run the download script to obtain them.
- See the README in `src/scene_detection/transnetv2_repo/inference-pytorch/` for details.

**Hardware-agnostic loading:**
The model loading code now detects CUDA availability and loads the model accordingly:
```python
import torch
from transnetv2_pytorch import TransNetV2
model = TransNetV2()
state_dict = torch.load("transnetv2-pytorch-weights.pth", map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))
model.load_state_dict(state_dict)
model.eval()
if torch.cuda.is_available():
    model.cuda()
``` 